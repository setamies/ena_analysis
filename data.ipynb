{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yfinance as yf\n",
    "import dotenv\n",
    "import os\n",
    "import utils\n",
    "\n",
    "# Import specific client handling for Dune analytics\n",
    "from dune_client.types import QueryParameter\n",
    "from dune_client.client import DuneClient\n",
    "from dune_client.query import QueryBase\n",
    "from flipside import Flipside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .env environment variables\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Setup Dune Analytics API client\n",
    "API_KEY = os.getenv('API_KEY_TWO') \n",
    "FLIPSIDE_API_KEY = os.getenv('FLIPSIDE_API_KEY')\n",
    "dune = DuneClient(api_key=API_KEY, base_url=\"https://api.dune.com\", request_timeout=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# Download historical price data for ETH and ENA from Yahoo Finance\n",
    "eth_hourly = yf.download('ETH-USD', start='2024-04-02', end='2024-07-22', interval='1h')['Close']\n",
    "ena_hourly = yf.download('ENA-USD', start='2024-04-02', end='2024-07-22', interval='1h')['Close']\n",
    "\n",
    "# Trim the dataset to make make ETH and ENA data equally large\n",
    "\n",
    "start_time = pd.Timestamp('2024-04-02 10:00:00', tz='UTC')\n",
    "\n",
    "# Trim the data to start from the specified timezone-aware time\n",
    "eth_hourly = eth_hourly[eth_hourly.index >= start_time]\n",
    "ena_hourly = ena_hourly[ena_hourly.index >= start_time]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate hourly returns for ETH and ENA\n",
    "eth_hourly_returns = eth_hourly.pct_change()\n",
    "ena_hourly_returns = ena_hourly.pct_change()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nquery = QueryBase(\\n    query_id=3584284 # https://dune.com/queries/3568055 -> DEX Pair Stats\\n)\\n\\nquery_result = dune.get_latest_result_dataframe(\\n    query=query\\n    , filters=\"Calimed > 1000\"\\n    , sort_by=[\"Calimed desc\"]\\n) \\n\\nquery_result.drop(columns=[\\'Debank\\', \\'Arkham\\', \\'ENS\\', \\'Address\\'], inplace=True)\\nquery_result.rename(columns={\\'Calimed\\': \\'Claimed\\'}, inplace=True)\\n\\n# to csv\\nquery_result.to_csv(\\'ena_claimed_data.csv\\', index=False)\\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data of how ENA airdrop claimers\n",
    "\"\"\"\n",
    "query = QueryBase(\n",
    "    query_id=3584284 # https://dune.com/queries/3568055 -> DEX Pair Stats\n",
    ")\n",
    "\n",
    "query_result = dune.get_latest_result_dataframe(\n",
    "    query=query\n",
    "    , filters=\"Calimed > 1000\"\n",
    "    , sort_by=[\"Calimed desc\"]\n",
    ") \n",
    "\n",
    "query_result.drop(columns=['Debank', 'Arkham', 'ENS', 'Address'], inplace=True)\n",
    "query_result.rename(columns={'Calimed': 'Claimed'}, inplace=True)\n",
    "\n",
    "# to csv\n",
    "query_result.to_csv('ena_claimed_data.csv', index=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nquery_id = 3583363  # This is a hypothetical ID for the query\\nquery_result_df = dune.get_latest_result_dataframe(query=query_id)\\nquery_result_df['Datetime'] = pd.to_datetime(query_result_df['block_hour'])\\nquery_result_df['Date'] = query_result_df['Datetime'].dt.date\\ndaily_claimed_tokens = query_result_df.groupby('Date')['claimed'].sum().reset_index()\\n\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fetch and process airdrop claim data from Dune Analytics\n",
    "\"\"\"\n",
    "query_id = 3583363  # This is a hypothetical ID for the query\n",
    "query_result_df = dune.get_latest_result_dataframe(query=query_id)\n",
    "query_result_df['Datetime'] = pd.to_datetime(query_result_df['block_hour'])\n",
    "query_result_df['Date'] = query_result_df['Datetime'].dt.date\n",
    "daily_claimed_tokens = query_result_df.groupby('Date')['claimed'].sum().reset_index()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Pendle ENA yield hourly data\n",
    "ena_hourly_yield = pd.read_csv('data/ENA_fixed_yield_hourly.csv')\n",
    "\n",
    "# Convert 'time' to datetime with timezone, and then convert to UTC\n",
    "ena_hourly_yield['time'] = pd.to_datetime(ena_hourly_yield['time'], format='%a %b %d %Y %H:%M:%S GMT%z (Eastern European Summer Time)')\n",
    "ena_hourly_yield['time'] = ena_hourly_yield['time'].dt.tz_convert('UTC')\n",
    "\n",
    "# Rename columns\n",
    "ena_hourly_yield.rename(columns={'time': 'Datetime', 'impliedApy': 'Implied_APY'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize `Flipside` with your API Key and API Url\n",
    "flipside = Flipside(FLIPSIDE_API_KEY, \"https://api-v2.flipsidecrypto.xyz\")\n",
    "sql = \"\"\"\n",
    "with\n",
    "\n",
    "A as(       select 'ethereum' as chain, origin_from_address as user, from_address as contract, to_address   as pool,     amount as amount, contract_address, block_timestamp from ethereum.core.ez_token_transfers \n",
    "union all   select 'ethereum' as chain, origin_to_address   as user, to_address   as contract, from_address as pool, 0 - amount as amount, contract_address, block_timestamp from ethereum.core.ez_token_transfers \n",
    "),\n",
    "\n",
    "-- stake\n",
    "B as(\n",
    "select    date_trunc('day',block_timestamp) as time, 'Manual stake' as action, sum(amount) as stake\n",
    "from      A \n",
    "where     contract_address = '0x57e114b691db790c35207b2e685d4a43181e6061'\n",
    "and       pool in ('0x8707f238936c12c309bfc2b9959c35828acfc512')\n",
    "group by  1,2),\n",
    "\n",
    "-- fill gap\n",
    "T1 as (select distinct time  from B),\n",
    "T2 as (select distinct action from B),\n",
    "T as( select time, action from T1, T2 group by 1,2),\n",
    "\n",
    "C as(\n",
    "select    a.time, a.action, case when stake is null then 0 else stake end as stake\n",
    "from      T as a \n",
    "left join B as b on a.time = b.time and a.action = b.action),\n",
    "\n",
    "D as(\n",
    "select    time, action, \n",
    "          stake,\n",
    "          sum(stake) over(partition by action order by time) as stake_action,\n",
    "          sum(stake) over(order by time) as stake_total\n",
    "from      C),\n",
    "\n",
    "P as(     \n",
    "select    date(hour) as date, token_address, avg(price) as price \n",
    "from       ethereum.price.ez_prices_hourly\n",
    "where     token_address  = '0x57e114b691db790c35207b2e685d4a43181e6061'\n",
    "group by  1,2 ),\n",
    "\n",
    "-- total supply\n",
    "AC as(     select from_address as contract,     amount as amount, contract_address, block_timestamp from ethereum.core.ez_token_transfers \n",
    "union all  select to_address   as contract, 0 - amount as amount, contract_address, block_timestamp from ethereum.core.ez_token_transfers \n",
    "),\n",
    "\n",
    "BC as(\n",
    "select    sum(amount) as amount\n",
    "from      AC\n",
    "where     contract_address = '0x57e114b691db790c35207b2e685d4a43181e6061'\n",
    "and       contract in ('0x0000000000000000000000000000000000000000')\n",
    "),\n",
    "\n",
    "E as(\n",
    "select    'ENA' as token, 'Staking TVL' as pool,\n",
    "           time, price,\n",
    "           stake,        price*stake        as tvl_daily,\n",
    "           stake_total,  price*stake_total  as tvl_total\n",
    "from       D  as a \n",
    "left join  P  as b on date(a.time) = b.date )\n",
    "\n",
    "\n",
    "\n",
    "select    *,\n",
    "          case when time = (select max(time) from E) then stake_total else null end as pie,\n",
    "          case when time = (select max(time) from E) then round(stake_total/(select amount from BC)*100,4) end as supply_rate\n",
    "from      E\n",
    "order by  time desc\n",
    "\"\"\"\n",
    "\n",
    "# Run the query against Flipside's query engine and await the results\n",
    "query_result_set = flipside.query(sql)\n",
    "\n",
    "# Function to auto-paginate query results from flipside\n",
    "def auto_paginate_result(query_result_set, page_size=10000):\n",
    "    \"\"\"\n",
    "    This function auto-paginates a query result to get all the data. It assumes 10,000 rows per page.\n",
    "    In case of an error, reduce the page size. Uses numpy.\n",
    "    \"\"\"\n",
    "    num_rows = query_result_set.page.totalRows\n",
    "    page_count = np.ceil(num_rows / page_size).astype(int)\n",
    "    all_rows = []\n",
    "    current_page = 1\n",
    "    while current_page <= page_count:\n",
    "        results = flipside.get_query_results(\n",
    "            query_result_set.query_id,\n",
    "            page_number=current_page,\n",
    "            page_size=page_size\n",
    "        )\n",
    "\n",
    "        if results.records:\n",
    "            all_rows.extend(results.records)  # Use extend() to add list elements\n",
    "\n",
    "        current_page += 1  # Increment the current page number\n",
    "\n",
    "    return all_rows  # Return all_rows in JSON format\n",
    "\n",
    "ena_stake_data_json = auto_paginate_result(query_result_set)\n",
    "# Create a new json file with the results\n",
    "\n",
    "ena_stake_data = pd.DataFrame(ena_stake_data_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "ena_stake_data_daily = ena_stake_data[['time', 'stake', 'stake_total']].rename(columns={'time': 'Datetime', 'stake': 'Stake', 'stake_total': 'Total_Stake'})\n",
    "\n",
    "# Turn the datetime column into a datetime object\n",
    "ena_stake_data_daily['Datetime'] = pd.to_datetime(ena_stake_data_daily['Datetime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to create an hourly df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing and estimating ENA claims\n",
    "ena_claims  = pd.read_csv('data/ena_1h_data.csv')[['Datetime', 'claimed', 'total_claimed']]\n",
    "ena_claims # Add days all the way to end_date\n",
    "ena_claims['Datetime'] = pd.to_datetime(ena_claims['Datetime'])\n",
    "\n",
    "# Convert the Datetime column to datetime objects and ensure it's timezone-aware\n",
    "ena_claims['Datetime'] = pd.to_datetime(ena_claims['Datetime'])\n",
    "if ena_claims['Datetime'].dt.tz is None:\n",
    "    ena_claims['Datetime'] = ena_claims['Datetime'].dt.tz_localize('UTC')\n",
    "\n",
    "# Create a date range from the last date in the DataFrame to 2024-07-23\n",
    "last_date = ena_claims['Datetime'].max()\n",
    "end_date = pd.Timestamp('2024-07-23', tz='UTC')\n",
    "\n",
    "# Generate the date range\n",
    "date_range = pd.date_range(start=last_date + pd.Timedelta(hours=1), end=end_date, freq='H')\n",
    "\n",
    "# Create a DataFrame for the new dates with NaN values for claimed and total_claimed\n",
    "new_data = pd.DataFrame({\n",
    "    'Datetime': date_range,\n",
    "    'claimed': np.nan,\n",
    "    'total_claimed': np.nan\n",
    "})\n",
    "\n",
    "# Append the new data to the existing DataFrame\n",
    "ena_claims_extended = pd.concat([ena_claims, new_data], ignore_index=True)\n",
    "\n",
    "# Ensure the DataFrame is sorted by Datetime\n",
    "ena_claims_extended = ena_claims_extended.sort_values(by='Datetime').reset_index(drop=True)\n",
    "\n",
    "# Forward fill the 'claimed' column with the last available value\n",
    "ena_claims_extended['claimed'] = ena_claims_extended['claimed'].ffill()\n",
    "\n",
    "# Calculate 'total_claimed' as a cumulative sum of 'claimed'\n",
    "ena_claims_extended['total_claimed'] = ena_claims_extended['claimed'].cumsum()\n",
    "\n",
    "query_result_df = ena_claims_extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hourly = pd.DataFrame({\n",
    "    'Datetime': eth_hourly.index,\n",
    "    'ETH_Price': eth_hourly.values,\n",
    "    'ETH_Returns': eth_hourly_returns.values,\n",
    "    'ENA_Price': ena_hourly.values,\n",
    "    'ENA_Returns': ena_hourly_returns.values\n",
    "})\n",
    "\n",
    "df_hourly = df_hourly.merge(query_result_df[['Datetime', 'claimed', 'total_claimed']], on='Datetime', how='left')\n",
    "df_hourly = df_hourly.merge(ena_hourly_yield[['Datetime', 'Implied_APY']], on='Datetime', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the hourly data to csv\n",
    "df_hourly.to_csv('data/ena_1h_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create a 1 day and 4h df from the 1h df\n",
    "df_4h = df_hourly.resample('4H', on='Datetime').agg({\n",
    "    'ETH_Price': 'last',\n",
    "    'ETH_Returns': 'sum',\n",
    "    'ENA_Price': 'last',\n",
    "    'ENA_Returns': 'sum',\n",
    "    'claimed': 'sum',\n",
    "    'total_claimed': 'last',\n",
    "    'Implied_APY': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "df_1d = df_hourly.resample('1D', on='Datetime').agg({\n",
    "    'ETH_Price': 'last',\n",
    "    'ETH_Returns': 'sum',\n",
    "    'ENA_Price': 'last',\n",
    "    'ENA_Returns': 'sum',\n",
    "    'claimed': 'sum',\n",
    "    'total_claimed': 'last',\n",
    "    'Implied_APY': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Store both of these dataframes to CSV\n",
    "df_1d.to_csv('data/ena_1d_data.csv', index=False)\n",
    "df_4h.to_csv('data/ena_4h_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ena_1d_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_187993/923969493.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Add the stake data to the 1d df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_1d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ena_1d_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Turn the datetime column into a datetime object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_1d\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Datetime'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_1d\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Datetime'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ena_1d_data.csv'"
     ]
    }
   ],
   "source": [
    "# Add the stake data to the 1d df\n",
    "df_1d = pd.read_csv('data/ena_1d_data.csv')\n",
    "\n",
    "# Turn the datetime column into a datetime object\n",
    "df_1d['Datetime'] = pd.to_datetime(df_1d['Datetime'])\n",
    "\n",
    "df_1d = df_1d.merge(ena_stake_data_daily, on='Datetime', how='left')\n",
    "\n",
    "# Store the final 1d df to csv\n",
    "df_1d.to_csv('ena_1d_data.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
